"""
Typed metrics collection utilities used by the validation tests.

The original project contained an extensive (and largely autogenerated)
metrics subsystem.  To keep things maintainable while satisfying the current
test-suite, we provide a compact, well-typed implementation that exposes the
behaviour the tests rely on: collecting validation/pipeline metrics,
generating simple summaries, and surfacing aggregate health indicators for the
dashboard and reporting layers.
"""

from __future__ import annotations

from collections.abc import Sequence
from dataclasses import dataclass
from datetime import UTC, datetime, timedelta
from enum import Enum
from statistics import mean
from typing import ClassVar

from src.type_definitions.common import JSONObject

from ..gates.quality_gate import GateResult
from ..rules.base_rules import ValidationResult

Number = float | int


class MetricType(Enum):
    """Supported metric types."""

    COUNTER = "counter"
    GAUGE = "gauge"
    TIMER = "timer"


@dataclass
class MetricValue:
    """Concrete metric sample."""

    name: str
    value: float
    metric_type: MetricType
    timestamp: datetime
    tags: dict[str, str]


@dataclass
class MetricSummary:
    """Aggregate statistics for a metric series."""

    name: str
    count: int
    average: float
    minimum: float
    maximum: float
    percentiles: dict[str, float]
    time_range_hours: int


@dataclass
class AlertRecord:
    """Alert raised when metric thresholds are exceeded."""

    metric: str
    value: float
    threshold: float
    timestamp: datetime
    tags: dict[str, str]

    def as_json(self) -> JSONObject:
        tags_json: JSONObject = dict(self.tags)
        return {
            "metric": self.metric,
            "value": self.value,
            "threshold": self.threshold,
            "timestamp": self.timestamp.isoformat(),
            "tags": tags_json,
        }


class MetricsCollector:
    """Lightweight in-memory metrics collector."""

    def __init__(self, retention_hours: int = 168) -> None:
        self._retention = timedelta(hours=retention_hours)
        self._metrics: dict[str, list[MetricValue]] = {}
        self._alerts: list[AlertRecord] = []
        self._thresholds: dict[str, float] = {
            "validation.error_rate": 0.1,
            "validation.quality_score": 0.75,
            "pipeline.execution_time": 300.0,
        }
        MetricsCollector._default_instance = self

    _default_instance: ClassVar[MetricsCollector | None] = None

    @classmethod
    def get_default_instance(cls) -> MetricsCollector | None:
        return cls._default_instance

    # ------------------------------------------------------------------ #
    # Public collection helpers
    # ------------------------------------------------------------------ #

    def collect_validation_metrics(
        self,
        results: ValidationResult | Sequence[ValidationResult],
        pipeline_name: str = "default",
        entity_type: str = "unknown",
    ) -> None:
        samples = [results] if isinstance(results, ValidationResult) else list(results)
        tags = {"pipeline": pipeline_name, "entity_type": entity_type}

        for result in samples:
            self.record_metric(
                "validation.quality_score",
                result.score,
                MetricType.GAUGE,
                tags,
            )

            severity_counts: dict[str, int] = {"error": 0, "warning": 0, "info": 0}
            for issue in result.issues:
                key = issue.severity.name.lower()
                severity_counts.setdefault(key, 0)
                severity_counts[key] += 1

            total_issues = sum(severity_counts.values())
            error_rate = (
                (severity_counts.get("error", 0) / total_issues)
                if total_issues
                else 0.0
            )
            self.record_metric(
                "validation.error_rate",
                error_rate,
                MetricType.GAUGE,
                tags,
            )

            for severity, count in severity_counts.items():
                if count:
                    self.record_metric(
                        f"validation.issues.{severity}",
                        float(count),
                        MetricType.COUNTER,
                        tags,
                    )

            status_value = 1.0 if result.is_valid else 0.0
            self.record_metric(
                "validation.status",
                status_value,
                MetricType.GAUGE,
                tags,
            )

    def collect_pipeline_metrics(
        self,
        pipeline_name: str,
        execution_time: float,
        entities_processed: int,
        quality_score: float,
        error_count: int,
        warning_count: int,
    ) -> None:
        tags = {"pipeline": pipeline_name}
        throughput = entities_processed / max(execution_time, 0.001)
        error_rate = error_count / max(entities_processed, 1)

        self.record_metric(
            "pipeline.execution_time",
            execution_time,
            MetricType.TIMER,
            tags,
        )
        self.record_metric(
            "pipeline.entities_processed",
            float(entities_processed),
            MetricType.COUNTER,
            tags,
        )
        self.record_metric("pipeline.throughput", throughput, MetricType.GAUGE, tags)
        self.record_metric(
            "pipeline.quality_score",
            quality_score,
            MetricType.GAUGE,
            tags,
        )
        self.record_metric(
            "pipeline.error_count",
            float(error_count),
            MetricType.COUNTER,
            tags,
        )
        self.record_metric(
            "pipeline.warning_count",
            float(warning_count),
            MetricType.COUNTER,
            tags,
        )
        self.record_metric("pipeline.error_rate", error_rate, MetricType.GAUGE, tags)

    def collect_gate_metrics(
        self,
        gate_name: str,
        gate_result: GateResult,
        pipeline_name: str = "default",
    ) -> None:
        tags = {"pipeline": pipeline_name, "gate": gate_name}
        status_value = 1.0 if gate_result.passed else 0.0
        quality_score = float(gate_result.quality_score)
        evaluation_time = float(gate_result.evaluation_time)

        self.record_metric("gate.status", status_value, MetricType.GAUGE, tags)
        self.record_metric("gate.quality_score", quality_score, MetricType.GAUGE, tags)
        self.record_metric(
            "gate.evaluation_time",
            evaluation_time,
            MetricType.TIMER,
            tags,
        )

        for severity, count in gate_result.issue_counts.items():
            self.record_metric(
                f"gate.issues.{severity}",
                float(count),
                MetricType.COUNTER,
                tags,
            )

    # ------------------------------------------------------------------ #
    # Metric queries
    # ------------------------------------------------------------------ #

    def get_current_value(self, name: str) -> float | None:
        samples = self._metrics.get(name, [])
        return samples[-1].value if samples else None

    def get_metric_summary(
        self,
        name: str,
        time_range_hours: int = 24,
    ) -> MetricSummary | None:
        self._prune_expired()
        cutoff = datetime.now(UTC) - timedelta(hours=time_range_hours)
        samples = [
            sample
            for sample in self._metrics.get(name, [])
            if sample.timestamp >= cutoff
        ]
        if not samples:
            return None

        values = [sample.value for sample in samples]
        sorted_values = sorted(values)
        count = len(values)
        percentiles = {
            "50": sorted_values[int(0.5 * (count - 1))],
            "90": sorted_values[int(0.9 * (count - 1))],
        }

        return MetricSummary(
            name=name,
            count=count,
            average=mean(values),
            minimum=min(values),
            maximum=max(values),
            percentiles=percentiles,
            time_range_hours=time_range_hours,
        )

    def get_system_health_score(self) -> float:
        quality = self.get_metric_summary("validation.quality_score")
        error_rate = self.get_metric_summary("validation.error_rate")

        score = 1.0
        if quality:
            score = max(0.0, min(quality.average, 1.0))

        if error_rate:
            score *= max(0.0, 1.0 - error_rate.average)

        return round(score, 4)

    def get_alerts(self, time_range_hours: int = 1) -> list[JSONObject]:
        cutoff = datetime.now(UTC) - timedelta(hours=time_range_hours)
        return [alert.as_json() for alert in self._alerts if alert.timestamp >= cutoff]

    def get_performance_report(self, time_range_hours: int = 24) -> JSONObject:
        quality = self.get_metric_summary("validation.quality_score", time_range_hours)
        error = self.get_metric_summary("validation.error_rate", time_range_hours)
        throughput = self.get_metric_summary("pipeline.throughput", time_range_hours)

        return {
            "health_score": self.get_system_health_score(),
            "metrics": {
                "quality_score": quality.average if quality else None,
                "error_rate": error.average if error else None,
                "throughput": throughput.average if throughput else None,
            },
            "time_range_hours": time_range_hours,
        }

    # ------------------------------------------------------------------ #
    # Recording utilities
    # ------------------------------------------------------------------ #

    def record_metric(
        self,
        name: str,
        value: Number,
        metric_type: MetricType,
        tags: dict[str, str] | None = None,
    ) -> None:
        tags = tags or {}
        sample = MetricValue(
            name=name,
            value=float(value),
            metric_type=metric_type,
            timestamp=datetime.now(UTC),
            tags=tags,
        )
        self._metrics.setdefault(name, []).append(sample)
        self._prune_expired()
        self._maybe_record_alert(sample)

    def increment_counter(
        self,
        name: str,
        tags: dict[str, str] | None = None,
        increment: Number = 1,
    ) -> None:
        self.record_metric(name, float(increment), MetricType.COUNTER, tags)

    # ------------------------------------------------------------------ #
    # Internal helpers
    # ------------------------------------------------------------------ #

    def _prune_expired(self) -> None:
        cutoff = datetime.now(UTC) - self._retention
        for name, samples in list(self._metrics.items()):
            filtered = [sample for sample in samples if sample.timestamp >= cutoff]
            if filtered:
                self._metrics[name] = filtered
            else:
                self._metrics.pop(name, None)

    def _maybe_record_alert(self, sample: MetricValue) -> None:
        threshold = self._thresholds.get(sample.name)
        if threshold is None:
            return

        should_alert = False
        if (
            sample.name.endswith("error_rate")
            or sample.name == "pipeline.execution_time"
        ):
            should_alert = sample.value > threshold
        elif sample.name.endswith("quality_score"):
            should_alert = sample.value < threshold

        if should_alert:
            self._alerts.append(
                AlertRecord(
                    metric=sample.name,
                    value=sample.value,
                    threshold=threshold,
                    timestamp=sample.timestamp,
                    tags=sample.tags,
                ),
            )


__all__ = ["MetricSummary", "MetricType", "MetricValue", "MetricsCollector"]
